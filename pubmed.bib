@Article{masuyama2017kernel,
  title = {Kernel Bayesian ART and ARTMAP},
  author = {Naoki Masuyama and Chu Kiong Loo and Farhan Dawood},
  year = {2017},
  month = {Nov},
  journal = {Neural networks : the official journal of the International Neural Network Society},
  volume = {98},
  pages = {76-86},
  eprint = {29202265},
  doi = {10.1016/j.neunet.2017.11.003},
  language = {eng},
  issn = {1879-2782},
  abstract = {Adaptive Resonance Theory (ART) is one of the successful approaches to resolving "the plasticity-stability dilemma" in neural networks, and its supervised learning model called ARTMAP is a powerful tool for classification. Among several improvements, such as Fuzzy or Gaussian based models, the state of art model is Bayesian based one, while solving the drawbacks of others. However, it is known that the Bayesian approach for the high dimensional and a large number of data requires high computational cost, and the covariance matrix in likelihood becomes unstable. This paper introduces Kernel Bayesian ART (KBA) and ARTMAP (KBAM) by integrating Kernel Bayes' Rule (KBR) and Correntropy Induced Metric (CIM) to Bayesian ART (BA) and ARTMAP (BAM), respectively, while maintaining the properties of BA and BAM. The kernel frameworks in KBA and KBAM are able to avoid the curse of dimensionality. In addition, the covariance-free Bayesian computation by KBR provides the efficient and stable computational capability to KBA and KBAM. Furthermore, Correntropy-based similarity measurement allows improving the noise reduction ability even in the high dimensional space. The simulation experiments show that KBA performs an outstanding self-organizing capability than BA, and KBAM provides the superior classification ability than BAM, respectively.},
  eprinttype = {pubmed},
}

@Article{dong2017improved,
  title = {An Improved Random Walker with Bayes Model for Volumetric Medical Image Segmentation},
  author = {Chunhua Dong and Xiangyan Zeng and Lanfen Lin and Hongjie Hu and Xianhua Han and Masoud Naghedolfeizi and Dawit Aberra and Yen-Wei Chen},
  year = {2017},
  journal = {Journal of healthcare engineering},
  volume = {2017},
  pages = {6506049},
  eprint = {29201332},
  doi = {10.1155/2017/6506049},
  language = {eng},
  issn = {2040-2295},
  abstract = {Random walk (RW) method has been widely used to segment the organ in the volumetric medical image. However, it leads to a very large-scale graph due to a number of nodes equal to a voxel number and inaccurate segmentation because of the unavailability of appropriate initial seed point setting. In addition, the classical RW algorithm was designed for a user to mark a few pixels with an arbitrary number of labels, regardless of the intensity and shape information of the organ. Hence, we propose a prior knowledge-based Bayes random walk framework to segment the volumetric medical image in a slice-by-slice manner. Our strategy is to employ the previous segmented slice to obtain the shape and intensity knowledge of the target organ for the adjacent slice. According to the prior knowledge, the object/background seed points can be dynamically updated for the adjacent slice by combining the narrow band threshold (NBT) method and the organ model with a Gaussian process. Finally, a high-quality image segmentation result can be automatically achieved using Bayes RW algorithm. Comparing our method with conventional RW and state-of-the-art interactive segmentation methods, our results show an improvement in the accuracy for liver segmentation (p < 0.001).},
  eprinttype = {pubmed},
}

@Article{pazlinares2017spatio,
  title = {Spatio Temporal EEG Source Imaging with the Hierarchical Bayesian Elastic Net and Elitist Lasso Models},
  author = {Deirel Paz-Linares and Mayrim Vega-Hern{\a'a}ndez and Pedro A Rojas-L{\a'o}pez and Pedro A Vald{\a'e}s-Hern{\a'a}ndez and Eduardo Mart{\a'\i}nez-Montes and Pedro A Vald{\a'e}s-Sosa},
  year = {2017},
  journal = {Frontiers in neuroscience},
  volume = {11},
  pages = {635},
  eprint = {29200994},
  doi = {10.3389/fnins.2017.00635},
  language = {eng},
  issn = {1662-4548},
  abstract = {The estimation of EEG generating sources constitutes an Inverse Problem (IP) in Neuroscience. This is an ill-posed problem due to the non-uniqueness of the solution and regularization or prior information is needed to undertake Electrophysiology Source Imaging. Structured Sparsity priors can be attained through combinations of (L1 norm-based) and (L2 norm-based) constraints such as the Elastic Net (ENET) and Elitist Lasso (ELASSO) models. The former model is used to find solutions with a small number of smooth nonzero patches, while the latter imposes different degrees of sparsity simultaneously along different dimensions of the spatio-temporal matrix solutions. Both models have been addressed within the penalized regression approach, where the regularization parameters are selected heuristically, leading usually to non-optimal and computationally expensive solutions. The existing Bayesian formulation of ENET allows hyperparameter learning, but using the computationally intensive Monte Carlo/Expectation Maximization methods, which makes impractical its application to the EEG IP. While the ELASSO have not been considered before into the Bayesian context. In this work, we attempt to solve the EEG IP using a Bayesian framework for ENET and ELASSO models. We propose a Structured Sparse Bayesian Learning algorithm based on combining the Empirical Bayes and the iterative coordinate descent procedures to estimate both the parameters and hyperparameters. Using realistic simulations and avoiding the inverse crime we illustrate that our methods are able to recover complicated source setups more accurately and with a more robust estimation of the hyperparameters and behavior under different sparsity scenarios than classical LORETA, ENET and LASSO Fusion solutions. We also solve the EEG IP using data from a visual attention experiment, finding more interpretable neurophysiological patterns with our methods. The Matlab codes used in this work, including Simulations, Methods, Quality Measures and Visualization Routines are freely available in a public website.},
  eprinttype = {pubmed},
}

@Article{teng2017bayesian,
  title = {Bayesian Computation for Log-Gaussian Cox Processes: A Comparative Analysis of Methods},
  author = {Ming Teng and Farouk S Nathoo and Timothy D Johnson},
  year = {2017},
  journal = {Journal of statistical computation and simulation},
  volume = {87},
  number = {11},
  pages = {2227-2252},
  eprint = {29200537},
  doi = {10.1080/00949655.2017.1326117},
  language = {eng},
  issn = {0094-9655},
  abstract = {The Log-Gaussian Cox Process is a commonly used model for the analysis of spatial point pattern data. Fitting this model is difficult because of its doubly-stochastic property, i.e., it is an hierarchical combination of a Poisson process at the first level and a Gaussian Process at the second level. Various methods have been proposed to estimate such a process, including traditional likelihood-based approaches as well as Bayesian methods. We focus here on Bayesian methods and several approaches that have been considered for model fitting within this framework, including Hamiltonian Monte Carlo, the Integrated nested Laplace approximation, and Variational Bayes. We consider these approaches and make comparisons with respect to statistical and computational efficiency. These comparisons are made through several simulation studies as well as through two applications, the first examining ecological data and the second involving neuroimaging data.},
  eprinttype = {pubmed},
}

@Article{gronau2017tutorial,
  title = {A tutorial on bridge sampling},
  author = {Quentin F Gronau and Alexandra Sarafoglou and Dora Matzke and Alexander Ly and Udo Boehm and Maarten Marsman and David S Leslie and Jonathan J Forster and Eric-Jan Wagenmakers and Helen Steingroever},
  year = {2017},
  month = {Dec},
  journal = {Journal of mathematical psychology},
  volume = {81},
  pages = {80-97},
  eprint = {29200501},
  doi = {10.1016/j.jmp.2017.09.005},
  language = {eng},
  issn = {0022-2496},
  abstract = {The marginal likelihood plays an important role in many areas of Bayesian statistics such as parameter estimation, model comparison, and model averaging. In most applications, however, the marginal likelihood is not analytically tractable and must be approximated using numerical methods. Here we provide a tutorial on bridge sampling (Bennett, 1976; Meng & Wong, 1996), a reliable and relatively straightforward sampling method that allows researchers to obtain the marginal likelihood for models of varying complexity. First, we introduce bridge sampling and three related sampling methods using the beta-binomial model as a running example. We then apply bridge sampling to estimate the marginal likelihood for the Expectancy Valence (EV) model-a popular model for reinforcement learning. Our results indicate that bridge sampling provides accurate estimates for both a single participant and a hierarchical version of the EV model. We conclude that bridge sampling is an attractive method for mathematical psychologists who typically aim to approximate the marginal likelihood for a limited set of possibly high-dimensional models.},
  eprinttype = {pubmed},
}

@Article{solanozavaleta2017species,
  title = {Species limits in the Morelet's Alligator Lizard (Anguidae: Gerrhonotinae)},
  author = {Israel Solano-Zavaleta and Adri{\a'a}n Nieto-Montes {de Oca}},
  year = {2017},
  month = {Nov},
  journal = {Molecular phylogenetics and evolution},
  eprint = {29199107},
  doi = {10.1016/j.ympev.2017.11.011},
  language = {eng},
  issn = {1095-9513},
  abstract = {The widely distributed, Central American anguid lizard Mesaspis moreletii is currently recognized as a polytypic species with five subspecies (M. m. fulvus, M. m. moreletii, M. m. rafaeli, M. m. salvadorensis, and M. m. temporalis). We reevaluated the species limits within Mesaspis moreletii using DNA sequences of one mitochondrial and three nuclear genes. The multi-locus data set included samples of all of the subspecies of M. moreletii, the other species of Mesaspis in Central America (M. cuchumatanus and M. monticola), and some populations assignable to M. moreletii but of uncertain subspecific identity from Honduras and Nicaragua. We first used a tree-based method for delimiting species based on mtDNA data to identify potential evolutionary independent lineages, and then analized the multilocus dataset with two species delimitation methods that use the multispecies coalescent model to evaluate different competing species delimitation models: the Bayes Factors species delimitation method (BFD) implemented in ∗BEAST, and the Bayesian Phylogenetics and Phylogeography (BP&P) method. Our results suggest that M. m. moreletii, M. m. rafaeli, M. m. salvadorensis, and M. m. temporalis represent distinct evolutionary independent lineages, and that the populations of uncertain status from Honduras and Nicaragua may represent additional undescribed species. Our results also suggest that M. m. fulvus is a synonym of M. m. moreletii. The biogeography of the Central American lineages of Mesaspis is discussed.},
  eprinttype = {pubmed},
}

@Article{tarasova2017based,
  title = {QNA-Based Prediction of Sites of Metabolism},
  author = {Olga Tarasova and Anastassia Rudik and Alexander Dmitriev and Alexey Lagunin and Dmitry Filimonov and Vladimir Poroikov},
  year = {2017},
  month = {Dec},
  journal = {Molecules (Basel, Switzerland)},
  volume = {22},
  number = {12},
  eprint = {29194399},
  doi = {10.3390/molecules22122123},
  language = {eng},
  issn = {1420-3049},
  abstract = {Metabolism of xenobiotics (Greek xenos: exogenous substances) plays an essential role in the prediction of biological activity and testing for the subsequent research and development of new drug candidates. Integration of various methods and techniques using different computational and experimental approaches is one of the keys to a successful metabolism prediction. While multiple structure-based and ligand-based approaches to metabolism prediction exist, the most important problem arises at the first stage of metabolism prediction: detection of the sites of metabolism (SOMs). In this paper, we describe the application of Quantitative Neighborhoods of Atoms (QNA) descriptors for prediction of the SOMs using potential function method, as well as several different machine learning techniques: naïve Bayes, random forest classifier, multilayer perceptron with back propagation and convolutional neural networks, and deep neural networks.},
  eprinttype = {pubmed},
}

@Article{liu2017interaction,
  title = {Interaction between Tobacco Smoking and Hepatitis B Virus Infection on the Risk of Liver Cancer in a Chinese Population},
  author = {Xing Liu and Aileen Baecker and Ming Wu and Jin-Yi Zhou and Jie Yang and Ren-Qiang Han and Pei-Hua Wang and Zi-Yi Jin and Ai-Min Liu and Xiaoping Gu and Xiao-Feng Zhang and Xu-Shan Wang and Ming Su and Xu Hu and Zheng Sun and Gang Li and Lina Mu and Na He and Liming Li and Jin-Kou Zhao and Zuo-Feng Zhang},
  year = {2017},
  month = {Nov},
  journal = {International journal of cancer},
  eprint = {29193051},
  doi = {10.1002/ijc.31181},
  language = {eng},
  issn = {1097-0215},
  abstract = {Although tobacco smoking has been reported as a risk factor for liver cancer, few studies have specifically explored the association among Chinese females and the potential interaction between smoking and other risk factors. A population-based case-control study was conducted and 2,011 liver cancer cases and 7,933 healthy controls were enrolled in Jiangsu, China from 2003 to 2010. Epidemiological data were collected, and serum HBsAg and anti-HCV antibody were measured. Unconditional logistic regression was used to examine association and potential interaction, while semi-Bayes method was employed to make estimates more conservative. The prevalence of serum HBsAg positivity was 43.2% among cases and 6.5% among controls. The adjusted odds ratio for ever smoking was 1.62 (95% CI: 1.33 - 1.96) among male and was 0.82 (95% CI: 0.53-1.26) among female. Age at first cigarette, duration of smoking and pack-years of smoking were all significantly associated with liver cancer among men. Compared to HBsAg negative never-smokers, the adjusted OR was 1.25 (95% CI: 1.03-1.52) for HBsAg-negative ever smokers, was 7.66 (95% CI: 6.05-9.71) for HBsAg-positive never smokers, and was 15.68 (95% CI: 12.06-20.39) for HBsAg-positive ever smokers. These different odds indicated super-additive (RERI: 7.77, 95% CI: 3.81-11.73) and super-multiplicative interactions (ROR: 1.64, 95% CI: 1.17-2.30) between HBV infection and tobacco smoking. Most associations and interactions detected remained statistically significant after semi-Bayes adjustments. Tobacco smoking and HBV infection positively interact in the development of liver cancer. This article is protected by copyright. All rights reserved.},
  eprinttype = {pubmed},
}

@Article{pradhan2017performance,
  title = {Performance comparison of first-order conditional estimation with interaction and Bayesian estimation methods for estimating the population parameters and its distribution from data sets with a low number of subjects},
  author = {Sudeep Pradhan and Byungjeong Song and Jaeyeon Lee and Jung-Woo Chae and Kyung Im Kim and Hyun-Moon Back and Nayoung Han and Kwang-Il Kwon and Hwi-Yeol Yun},
  year = {2017},
  month = {Dec},
  journal = {BMC medical research methodology},
  volume = {17},
  number = {1},
  pages = {154},
  eprint = {29191177},
  doi = {10.1186/s12874-017-0427-0},
  language = {eng},
  issn = {1471-2288},
  abstract = {BACKGROUND: Exploratory preclinical, as well as clinical trials, may involve a small number of patients, making it difficult to calculate and analyze the pharmacokinetic (PK) parameters, especially if the PK parameters show very high inter-individual variability (IIV). In this study, the performance of a classical first-order conditional estimation with interaction (FOCE-I) and expectation maximization (EM)-based Markov chain Monte Carlo Bayesian (BAYES) estimation methods were compared for estimating the population parameters and its distribution from data sets having a low number of subjects.
METHODS: In this study, 100 data sets were simulated with eight sampling points for each subject and with six different levels of IIV (5%, 10%, 20%, 30%, 50%, and 80%) in their PK parameter distribution. A stochastic simulation and estimation (SSE) study was performed to simultaneously simulate data sets and estimate the parameters using four different methods: FOCE-I only, BAYES(C) (FOCE-I and BAYES composite method), BAYES(F) (BAYES with all true initial parameters and fixed ω 2 ), and BAYES only. Relative root mean squared error (rRMSE) and relative estimation error (REE) were used to analyze the differences between true and estimated values. A case study was performed with a clinical data of theophylline available in NONMEM distribution media. NONMEM software assisted by Pirana, PsN, and Xpose was used to estimate population PK parameters, and R program was used to analyze and plot the results.
RESULTS: The rRMSE and REE values of all parameter (fixed effect and random effect) estimates showed that all four methods performed equally at the lower IIV levels, while the FOCE-I method performed better than other EM-based methods at higher IIV levels (greater than 30%). In general, estimates of random-effect parameters showed significant bias and imprecision, irrespective of the estimation method used and the level of IIV. Similar performance of the estimation methods was observed with theophylline dataset.
CONCLUSIONS: The classical FOCE-I method appeared to estimate the PK parameters more reliably than the BAYES method when using a simple model and data containing only a few subjects. EM-based estimation methods can be considered for adapting to the specific needs of a modeling project at later steps of modeling.},
  eprinttype = {pubmed},
}

@Article{parr2017computational,
  title = {The Computational Anatomy of Visual Neglect},
  author = {Thomas Parr and Karl J Friston},
  year = {2017},
  month = {Nov},
  journal = {Cerebral cortex (New York, N.Y. : 1991)},
  pages = {1-14},
  eprint = {29190328},
  doi = {10.1093/cercor/bhx316},
  language = {eng},
  issn = {1460-2199},
  abstract = {Visual neglect is a debilitating neuropsychological phenomenon that has many clinical implications and-in cognitive neuroscience-offers an important lesion deficit model. In this article, we describe a computational model of visual neglect based upon active inference. Our objective is to establish a computational and neurophysiological process theory that can be used to disambiguate among the various causes of this important syndrome; namely, a computational neuropsychology of visual neglect. We introduce a Bayes optimal model based upon Markov decision processes that reproduces the visual searches induced by the line cancellation task (used to characterize visual neglect at the bedside). We then consider 3 distinct ways in which the model could be lesioned to reproduce neuropsychological (visual search) deficits. Crucially, these 3 levels of pathology map nicely onto the neuroanatomy of saccadic eye movements and the systems implicated in visual neglect.},
  eprinttype = {pubmed},
}

@Article{awaysheh2017identifying,
  title = {Identifying free-text features to improve automated classification of structured histopathology reports for feline small intestinal disease},
  author = {Abdullah Awaysheh and Jeffrey Wilcke and Fran{\c c}ois Elvinger and Loren Rees and Weiguo Fan and Kurt Zimmerman},
  year = {2017},
  month = {Nov},
  journal = {Journal of veterinary diagnostic investigation : official publication of the American Association of Veterinary Laboratory Diagnosticians, Inc},
  pages = {1040638717744002},
  eprint = {29188759},
  doi = {10.1177/1040638717744002},
  language = {eng},
  issn = {1943-4936},
  abstract = {The histologic evaluation of gastrointestinal (GI) biopsies is the standard for diagnosis of a variety of GI diseases (e.g., inflammatory bowel disease [IBD] and alimentary lymphoma [ALA]). The World Small Animal Veterinary Association (WSAVA) Gastrointestinal International Standardization Group proposed a reporting standard for GI biopsies consisting of a defined set of microscopic features. We compared the machine classification accuracy of free-text microscopic findings with those represented in the WSAVA format with a diagnosis of IBD and ALA. Unstructured free-text duodenal biopsy pathology reports from cats ( n = 60) with a diagnosis of IBD ( n = 20), ALA ( n = 20), or normal ( n = 20) were identified. Biopsy samples from these cases were then scored following the WSAVA guidelines to create a set of structured reports. Three supervised machine-learning algorithms were trained using the structured and then the unstructured reports. Diagnosis classification accuracy for the 3 algorithms was compared using the structured and unstructured reports. Using naive Bayes and neural networks, unstructured information-based models achieved higher diagnostic accuracy (0.90 and 0.88, respectively) compared to the structured information-based models (0.74 and 0.72, respectively). Results suggest that discriminating diagnostic information was lost using current WSAVA microscopic guideline features. Addition of free-text features (number of plasma cells) increased WSAVA auto-classification performance. The methodologies reported in our study represent a way of identifying candidate microscopic features for use in structured histopathology reports.},
  eprinttype = {pubmed},
}

@Article{saadat2017predicting,
  title = {Predicting Quality of Life Changes in Hemodialysis Patients Using Machine Learning: Generation of an Early Warning System},
  author = {Shoab Saadat and Ayesha Aziz and Hira Ahmad and Hira Imtiaz and Zara S Sohail and Alvina Kazmi and Sanaa Aslam and Naveen Naqvi and Sidra Saadat},
  year = {2017},
  month = {Sep},
  journal = {Cureus},
  volume = {9},
  number = {9},
  pages = {e1713},
  eprint = {29188157},
  doi = {10.7759/cureus.1713},
  language = {eng},
  issn = {2168-8184},
  abstract = {Objective To predict changes in the quality of life scores of hemodialysis patients for the coming month and the development of an early warning system using machine learning Methods It was a prospective cohort study (one-month duration) at the dialysis center of a tertiary care hospital in Pakistan. The study started on 1st October 2016. About 78 patients have been enrolled till now. Bachelor of Medicine and Bachelor of Surgery (MBBS) qualified doctors administered a proforma with demographics and the validated Urdu version of World Health Organization Quality Of Life-BREF (WHOQOL-BREF). It was to be repeated after one month to the same patient by the same investigator. Simple statistics were computed using SPSS version 24 (IBM Corp., Armonk, NY) while machine learning was performed using R (version 3.0) and Orange (version 3.1). Results Using machine learning algorithms, two models (classification tree and Naïve Bayes) were generated to predict an increase or decrease of 5% in a patient's WHOQOL-BREF score over one month. The classification tree was selected as the most accurate model with an area under curve (AUC) of 83.3% (accuracy: 81.9%) for the prediction of 5% increase in QOL and an AUC of 76.2% (accuracy: 81.8%) for the prediction of 5% decrease in QOL over the coming month. The factors associated with an increase of QOL by 5% or more over the next month included younger age (<19 years) and higher iron sucrose doses (>278mg/month). Drops in psychological, physical, and social domain scores lead to a decrease of 5% or more in QOL scores over the following month. Conclusion An early warning system, dialysis data interpretation for algorithmic-prediction on quality of life (DIAL) was built for the early detection of deteriorating QOL scores in the hemodialysis population using machine learning algorithms. The model pointed out that working on psychological and environmental domains, in particular, may prevent the drop in QOL scores from occurring. DIAL, if implemented on a larger scale, is expected to help patients in terms of ensuring a better QOL and in reducing the financial burden in the long term.},
  eprinttype = {pubmed},
}

@Article{wang2017systematic,
  title = {Systematic analysis and prediction of type IV secreted effector proteins by machine learning approaches},
  author = {Jiawei Wang and Bingjiao Yang and Yi An and Tatiana Marquez-Lago and Andr{\a'e} Leier and Jonathan Wilksch and Qingyang Hong and Yang Zhang and Morihiro Hayashida and Tatsuya Akutsu and Geoffrey I Webb and Richard A Strugnell and Jiangning Song and Trevor Lithgow},
  year = {2017},
  month = {Nov},
  journal = {Briefings in bioinformatics},
  eprint = {29186295},
  doi = {10.1093/bib/bbx164},
  language = {eng},
  issn = {1477-4054},
  abstract = {In the course of infecting their hosts, pathogenic bacteria secrete numerous effectors, namely, bacterial proteins that pervert host cell biology. Many Gram-negative bacteria, including context-dependent human pathogens, use a type IV secretion system (T4SS) to translocate effectors directly into the cytosol of host cells. Various type IV secreted effectors (T4SEs) have been experimentally validated to play crucial roles in virulence by manipulating host cell gene expression and other processes. Consequently, the identification of novel effector proteins is an important step in increasing our understanding of host-pathogen interactions and bacterial pathogenesis. Here, we train and compare six machine learning models, namely, Naïve Bayes (NB), K-nearest neighbor (KNN), logistic regression (LR), random forest (RF), support vector machines (SVMs) and multilayer perceptron (MLP), for the identification of T4SEs using 10 types of selected features and 5-fold cross-validation. Our study shows that: (1) including different but complementary features generally enhance the predictive performance of T4SEs; (2) ensemble models, obtained by integrating individual single-feature models, exhibit a significantly improved predictive performance and (3) the 'majority voting strategy' led to a more stable and accurate classification performance when applied to predicting an ensemble learning model with distinct single features. We further developed a new method to effectively predict T4SEs, Bastion4 (Bacterial secretion effector predictor for T4SS), and we show our ensemble classifier clearly outperforms two recent prediction tools. In summary, we developed a state-of-the-art T4SE predictor by conducting a comprehensive performance evaluation of different machine learning algorithms along with a detailed analysis of single- and multi-feature selections.},
  eprinttype = {pubmed},
}

@Article{geary2017early,
  title = {Early Conceptual Understanding of Cardinality Predicts Superior School-Entry Number-System Knowledge},
  author = {David C Geary and Kristy vanMarle and Felicia W Chu and Jeffrey Rouder and Mary K Hoard and Lara Nugent},
  year = {2017},
  month = {Nov},
  journal = {Psychological science},
  pages = {956797617729817},
  eprint = {29185879},
  doi = {10.1177/0956797617729817},
  language = {eng},
  issn = {1467-9280},
  abstract = {We demonstrate a link between preschoolers' quantitative competencies and their school-entry knowledge of the relations among numbers (number-system knowledge). The quantitative competencies of 141 children (69 boys) were assessed at the beginning of preschool and throughout the next 2 years of preschool, as was their mathematics and reading achievement at the end of kindergarten and their number-system knowledge at the beginning of first grade. A combination of Bayes analyses and standard regressions revealed that the age at which the children had the conceptual insight that number words represent specific quantities (cardinal value) was strongly related to their later number-system knowledge and was more consistently related to broader mathematics than to reading achievement, controlling for intelligence, executive function, and parental education levels. The key implication is that it is not simply knowledge of cardinal value but the age of acquisition of this principle that is central to later mathematical development and school readiness.},
  eprinttype = {pubmed},
}

@Article{song2017using,
  title = {Using imputed genotype data in the joint score tests for genetic association and gene-environment interactions in case-control studies},
  author = {Minsun Song and William Wheeler and Neil E Caporaso and Maria Teresa Landi and Nilanjan Chatterjee},
  year = {2017},
  month = {Nov},
  journal = {Genetic epidemiology},
  eprint = {29178451},
  doi = {10.1002/gepi.22093},
  language = {eng},
  issn = {1098-2272},
  abstract = {Genome-wide association studies (GWAS) are now routinely imputed for untyped single nucleotide polymorphisms (SNPs) based on various powerful statistical algorithms for imputation trained on reference datasets. The use of predicted allele counts for imputed SNPs as the dosage variable is known to produce valid score test for genetic association. In this paper, we investigate how to best handle imputed SNPs in various modern complex tests for genetic associations incorporating gene-environment interactions. We focus on case-control association studies where inference for an underlying logistic regression model can be performed using alternative methods that rely on varying degree on an assumption of gene-environment independence in the underlying population. As increasingly large-scale GWAS are being performed through consortia effort where it is preferable to share only summary-level information across studies, we also describe simple mechanisms for implementing score tests based on standard meta-analysis of "one-step" maximum-likelihood estimates across studies. Applications of the methods in simulation studies and a dataset from GWAS of lung cancer illustrate ability of the proposed methods to maintain type-I error rates for the underlying testing procedures. For analysis of imputed SNPs, similar to typed SNPs, the retrospective methods can lead to considerable efficiency gain for modeling of gene-environment interactions under the assumption of gene-environment independence. Methods are made available for public use through CGEN R software package.},
  eprinttype = {pubmed},
}

@Article{morrison2017what,
  title = {What should a forensic practitioner's likelihood ratio be? II},
  author = {Geoffrey Stewart Morrison},
  year = {2017},
  month = {Nov},
  journal = {Science & justice : journal of the Forensic Science Society},
  volume = {57},
  number = {6},
  pages = {472-476},
  eprint = {29173462},
  doi = {10.1016/j.scijus.2017.08.004},
  language = {eng},
  issn = {1355-0306},
  abstract = {In the debate as to whether forensic practitioners should assess and report the precision of the strength of evidence statements that they report to the courts, I remain unconvinced by proponents of the position that only a subjectivist concept of probability is legitimate. I consider this position counterproductive for the goal of having forensic practitioners implement, and courts not only accept but demand, logically correct and scientifically valid evaluation of forensic evidence. In considering what would be the best approach for evaluating strength of evidence, I suggest that the desiderata be (1) to maximise empirically demonstrable performance; (2) to maximise objectivity in the sense of maximising transparency and replicability, and minimising the potential for cognitive bias; and (3) to constrain and make overt the forensic practitioner's subjective-judgement based decisions so that the appropriateness of those decisions can be debated before the judge in an admissibility hearing and/or before the trier of fact at trial. All approaches require the forensic practitioner to use subjective judgement, but constraining subjective judgement to decisions relating to selection of hypotheses, properties to measure, training and test data to use, and statistical modelling procedures to use - decisions which are remote from the output stage of the analysis - will substantially reduce the potential for cognitive bias. Adopting procedures based on relevant data, quantitative measurements, and statistical models, and directly reporting the output of the statistical models will also maximise transparency and replicability. A procedure which calculates a Bayes factor on the basis of relevant sample data and reference priors is no less objective than a frequentist calculation of a likelihood ratio on the same data. In general, a Bayes factor calculated using uninformative or reference priors will be closer to a value of 1 than a frequentist best estimate likelihood ratio. The bound closest to 1 based on a frequentist best estimate likelihood ratio and an assessment of its precision will also, by definition, be closer to a value of 1 than the frequentist best estimate likelihood ratio. From a practical perspective, both procedures shrink the strength of evidence value towards the neutral value of 1. A single-value Bayes factor or likelihood ratio may be easier for the courts to handle than a distribution. I therefore propose as a potential practical solution, the use of procedures which account for imprecision by shrinking the calculated Bayes factor or likelihood ratio towards 1, the choice of the particular procedure being based on empirical demonstration of performance.},
  eprinttype = {pubmed},
}

@Article{stahl2017fact,
  title = {Fact or fiction: reducing the proportion and impact of false positives},
  author = {D Stahl and A Pickles},
  year = {2017},
  month = {Nov},
  journal = {Psychological medicine},
  pages = {1-10},
  eprint = {29173233},
  doi = {10.1017/S003329171700294X},
  language = {eng},
  issn = {1469-8978},
  abstract = {False positive findings in science are inevitable, but are they particularly common in psychology and psychiatry? The evidence that we review suggests that while not restricted to our field, the problem is acute. We describe the concept of researcher 'degrees-of-freedom' to explain how many false-positive findings arise, and how the various strategies of registration, pre-specification, and reporting standards that are being adopted both reduce and make these visible. We review possible benefits and harms of proposed statistical solutions, from tougher requirements for significance, to Bayesian and machine learning approaches to analysis. Finally we consider the organisation and methods for replication and systematic review in psychology and psychiatry.},
  eprinttype = {pubmed},
}

@Article{vanerp2017prior,
  title = {Prior Sensitivity Analysis in Default Bayesian Structural Equation Modeling},
  author = {Sara {van Erp} and Joris Mulder and Daniel L Oberski},
  year = {2017},
  month = {Nov},
  journal = {Psychological methods},
  eprint = {29172613},
  doi = {10.1037/met0000162},
  language = {eng},
  issn = {1939-1463},
  abstract = {Bayesian structural equation modeling (BSEM) has recently gained popularity because it enables researchers to fit complex models and solve some of the issues often encountered in classical maximum likelihood estimation, such as nonconvergence and inadmissible solutions. An important component of any Bayesian analysis is the prior distribution of the unknown model parameters. Often, researchers rely on default priors, which are constructed in an automatic fashion without requiring substantive prior information. However, the prior can have a serious influence on the estimation of the model parameters, which affects the mean squared error, bias, coverage rates, and quantiles of the estimates. In this article, we investigate the performance of three different default priors: noninformative improper priors, vague proper priors, and empirical Bayes priors-with the latter being novel in the BSEM literature. Based on a simulation study, we find that these three default BSEM methods may perform very differently, especially with small samples. A careful prior sensitivity analysis is therefore needed when performing a default BSEM analysis. For this purpose, we provide a practical step-by-step guide for practitioners to conducting a prior sensitivity analysis in default BSEM. Our recommendations are illustrated using a well-known case study from the structural equation modeling literature, and all code for conducting the prior sensitivity analysis is available in the online supplemental materials. (PsycINFO Database Record},
  eprinttype = {pubmed},
}
